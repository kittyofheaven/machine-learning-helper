# NLP Section
This section containing all material of NLP 
## Resources
- [A Simple Introduction to Natural Language Processing](https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32)
- [How to solve 90% of NLP problems: a step-by-step guide](https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e)
### Data Preparing
- [TensorFlow text loading tutorial](https://www.tensorflow.org/tutorials/load_data/text)
- [Reading text files with Python](https://realpython.com/read-write-files-python/)
- [What are word embeddings?](https://machinelearningmastery.com/what-are-word-embeddings/) by Machine Learning Mastery.
- [word embeddings page on the TensorFlow website](https://www.tensorflow.org/tutorials/text/word_embeddings)
- https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf
### Conv1D
- [Understanding Convolutional Neural Networks for Text Classification](https://aclanthology.org/W18-5408.pdf) Pdf
- [Understanding Convolutional Neural Networks for Text Classification](https://arxiv.org/abs/1809.08037) Archive
- [CNN Explainer](https://poloclub.github.io/cnn-explainer/)  
- [The difference of padding on CNN](https://stackoverflow.com/questions/37674306/what-is-the-difference-between-same-and-valid-padding-in-tf-nn-max-pool-of-t) 
### RNN
- [MIT's Recurrent Neural Networks lecture](https://youtu.be/SEnXr6v2ifU)
- [The TensorFlow RNN guide](https://www.tensorflow.org/guide/keras/rnn)
### Some great blogpost
- Andrei Karpathy's [The Unreasonable Effectiveness of RNNs](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) dives into generating Shakespeare text with RNNs.
- [Text Classification with NLP: Tf-Idf vs Word2Vec vs BERT](https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794) by Mauro Di Pietro.
### Others
- [Mechanics of Seq2seq Models With Attention](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/). These are a foundational component of the transformer architecture and also often add improvments to deep NLP models.
- [Transformer architectures](http://jalammar.github.io/illustrated-transformer/). This model architecture has recently taken the NLP world by storm, achieving state of the art on many benchmarks. However, it does take a little more processing to get off the ground, the [HuggingFace Models (formerly HuggingFace Transformers) library](https://huggingface.co/models/) is probably your best quick start.
